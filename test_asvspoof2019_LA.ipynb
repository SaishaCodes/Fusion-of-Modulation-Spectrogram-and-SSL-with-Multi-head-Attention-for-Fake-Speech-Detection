{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from model import Model\n",
    "from core_scripts.startup_config import set_random_seed\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "import soundfile as sf\n",
    "from evaluation import calculate_tDCF_EER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(x, max_len=64600):\n",
    "    x_len = x.shape[0]\n",
    "    if x_len >= max_len:\n",
    "        return x[:max_len]\n",
    "    # need to pad\n",
    "    num_repeats = int(max_len / x_len) + 1\n",
    "    padded_x = np.tile(x, (1, num_repeats))[:, :max_len][0]\n",
    "    return padded_x\n",
    "\n",
    "\n",
    "def pad_random(x: np.ndarray, max_len: int = 64600):\n",
    "    x_len = x.shape[0]\n",
    "    # if duration is already long enough\n",
    "    if x_len >= max_len:\n",
    "        stt = np.random.randint(x_len - max_len)\n",
    "        return x[stt:stt + max_len]\n",
    "\n",
    "    # if too short\n",
    "    num_repeats = int(max_len / x_len) + 1\n",
    "    padded_x = np.tile(x, (num_repeats))[:max_len]\n",
    "    return padded_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genSpoof_list(dir_meta, is_train=False, is_eval=False):\n",
    "\n",
    "    d_meta = {}\n",
    "    file_list = []\n",
    "    with open(dir_meta, \"r\") as f:\n",
    "        l_meta = f.readlines()\n",
    "\n",
    "    if is_train:\n",
    "        for line in l_meta:\n",
    "            _, key, _, _, label = line.strip().split(\" \")\n",
    "            file_list.append(key)\n",
    "            d_meta[key] = 1 if label == \"bonafide\" else 0\n",
    "        return d_meta, file_list\n",
    "\n",
    "    elif is_eval:\n",
    "        for line in l_meta:\n",
    "            _, key, _, _, _ = line.strip().split(\" \")\n",
    "            #key = line.strip()\n",
    "            file_list.append(key)\n",
    "        return file_list\n",
    "    else:\n",
    "        for line in l_meta:\n",
    "            _, key, _, _, label = line.strip().split(\" \")\n",
    "            file_list.append(key)\n",
    "            d_meta[key] = 1 if label == \"bonafide\" else 0\n",
    "        return d_meta, file_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_ASVspoof2019_devNeval(Dataset):\n",
    "    def __init__(self, list_IDs, base_dir):\n",
    "        \"\"\"self.list_IDs\t: list of strings (each string: utt key),\n",
    "        \"\"\"\n",
    "        self.list_IDs = list_IDs\n",
    "        self.base_dir = base_dir\n",
    "        self.cut = 64600  # take ~4 sec audio (64600 samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        key = self.list_IDs[index]\n",
    "        X, _ = sf.read(str(self.base_dir + f\"flac/{key}.flac\"))\n",
    "        X_pad = pad(X, self.cut)\n",
    "        x_inp = Tensor(X_pad)\n",
    "        return x_inp, key\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    database_path = \"[path to dataset]\"\n",
    "    protocols_path = \"database/\"\n",
    "    seed = 1234\n",
    "    track = \"LA\"\n",
    "    is_eval = True\n",
    "    cudnn_deterministic_toggle = True\n",
    "    cudnn_benchmark_toggle = False\n",
    "    model_path = \"[path to model]\"\n",
    "    \n",
    "args = Arguments()\n",
    "eval_score_path = \"[output eval score path]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(args.seed, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track = args.track\n",
    "prefix      = 'ASVspoof_{}'.format(track)\n",
    "prefix_2019 = 'ASVspoof2019.{}'.format(track)\n",
    "prefix_2021 = 'ASVspoof2021.{}'.format(track)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'                  \n",
    "print('Device: {}'.format(device))\n",
    "\n",
    "model = Model(args,device)\n",
    "nb_params = sum([param.view(-1).size()[0] for param in model.parameters()])\n",
    "model =model.to(device)\n",
    "print('nb_params:',nb_params)\n",
    "\n",
    "if args.model_path:\n",
    "    model.load_state_dict(torch.load(args.model_path,map_location=device))\n",
    "    print('Model loaded : {}'.format(args.model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_database_path = args.database_path + \"ASVspoof2019_{}_eval/\".format(track)\n",
    "eval_trial_path = args.database_path+\"ASVspoof2019_{}_cm_protocols/{}.cm.eval.trl.txt\".format(\n",
    "            track, prefix_2019)\n",
    "file_eval = genSpoof_list(dir_meta=eval_trial_path,\n",
    "                              is_train=False,\n",
    "                              is_eval=True)\n",
    "eval_set = Dataset_ASVspoof2019_devNeval(list_IDs=file_eval,\n",
    "                                            base_dir=eval_database_path)\n",
    "eval_loader = DataLoader(eval_set,\n",
    "                            batch_size=24,\n",
    "                            shuffle=False,\n",
    "                            drop_last=False,\n",
    "                            pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_evaluation_file(\n",
    "    data_loader: DataLoader,\n",
    "    model,\n",
    "    device: torch.device,\n",
    "    save_path: str,\n",
    "    trial_path: str) -> None:\n",
    "    \"\"\"Perform evaluation and save the score to a file\"\"\"\n",
    "    model.eval()\n",
    "    with open(trial_path, \"r\") as f_trl:\n",
    "        trial_lines = f_trl.readlines()\n",
    "    fname_list = []\n",
    "    score_list = []\n",
    "    for batch_x, utt_id in data_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        with torch.no_grad():\n",
    "            batch_out = model(batch_x)\n",
    "            batch_score = (batch_out[:, 1]).data.cpu().numpy().ravel()\n",
    "        # add outputs\n",
    "        fname_list.extend(utt_id)\n",
    "        score_list.extend(batch_score.tolist())\n",
    "\n",
    "    assert len(trial_lines) == len(fname_list) == len(score_list)\n",
    "    with open(save_path, \"w\") as fh:\n",
    "        for fn, sco, trl in zip(fname_list, score_list, trial_lines):\n",
    "            _, utt_id, _, src, key = trl.strip().split(' ')\n",
    "            assert fn == utt_id\n",
    "            fh.write(\"{} {} {} {}\\n\".format(utt_id, src, key, sco))\n",
    "    print(\"Scores saved to {}\".format(save_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_evaluation_file(eval_loader, model, device,\n",
    "                        eval_score_path, eval_trial_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import calculate_tDCF_EER\n",
    "import os\n",
    "\n",
    "asv_scores_file = \"ASVspoof2019_LA_asv_scores/ASVspoof2019.LA.asv.eval.gi.trl.scores.txt\"\n",
    "eval_eer, eval_tdcf = calculate_tDCF_EER(\n",
    "            cm_scores_file=eval_score_path,\n",
    "            asv_score_file=args.database_path + asv_scores_file,\n",
    "            output_file=\"[output path]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spoof",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
